{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Sales Forecasting 7.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mnZ7cihRWN5b","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# The current version of seaborn generates a bunch of warnings that we'll ignore\n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","sns.set_style('darkgrid')\n","import os\n","import time\n","from datetime import datetime\n","#!pip install pendulum\n","#!pip install pyramid-arima\n","import pendulum\n","#from pyramid.arima import auto_arima\n","\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","import lightgbm as lgb \n","from sklearn.ensemble import GradientBoostingRegressor \n","from xgboost import XGBRegressor\n"," \n","#Directory needs to be changed \n","DIR = '/content/gdrive/My Drive/Sales Forecasting/' "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpEMN21OYL8p","colab_type":"code","outputId":"b86f5238-e2ea-48c1-e523-bc0789d1cb69","executionInfo":{"status":"ok","timestamp":1583278697257,"user_tz":360,"elapsed":17958,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"04844477878113654498"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vev6sMJwWN5j","colab_type":"code","colab":{}},"source":["def getData(folder_path, file_name):\n","    ext = file_name.split(\".\")[-1].lower()\n","    if ext == \"csv\":\n","        data = pd.read_csv(os.path.join(folder_path, file_name))\n","    elif ext == \"xlsx\":\n","        data = pd.read_excel(os.path.join(folder_path, file_name))\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48OTUpRcWN5m","colab_type":"code","colab":{}},"source":["#Load the dataset\n","sales_train = getData(DIR, 'train.csv')\n","features = getData(DIR, 'features.csv')\n","stores = getData(DIR, 'stores.csv')\n","sales_test = getData(DIR, 'test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bGkv97dGjCU","colab_type":"text"},"source":["# 1) Data Cleaning\n","\n","*   In time independent data (non-time-series), a common practice is to fill the gaps with the mean or median value of the field. However, this is not applicable in the time series. Missing values in columns \"CPI\" & \"Unemployment\" can be imputed using rolling average, interpolation techniques (linear, time, spline) & ARIMA. In this notebook, we use rolling average to impute missing values.\n","\n","*   Weekly sales column has negative values. We assume that this column can’t be negative, and hence convert them to positives.\n","\n","```\n","# Using ARIMA to impute missing values.\n","stores = list(range(1,46))\n","for store in stores:\n","  temp_df = features[features['Store'] == store][['Date','CPI']]\n","  temp_df.Date = pd.to_datetime(temp_df.Date)\n","  temp_df = temp_df.set_index('Date')\n","    \n","  # The problem with plain ARIMA model is that it does not account for seasonality.\n","  # If the time series exhibits seasonality, then, SARIMA is better as it uses seasonal differencing.\n","  ts_model = auto_arima(temp_df[:169], \n","                            start_p=0, start_q=0, start_P=0, start_Q=0,\n","                            max_p=3, max_q=3, # maximum p and q\n","                            max_P=3, max_Q=3, # maximum P and Q\n","                            m=12, \n","                            seasonal=True, # No seasonality \n","                            test='adf', # use Augmented Dickey-Fuller(adftest) test to find optimal 'd'\n","                            seasonal_test='ch', # use CHTest to find optimal 'D'\n","                            d=None, # let model determine 'd' \n","                            D=None, # let model determine 'D' \n","                            trace=True, error_action='ignore', suppress_warnings=True, stepwise=True)\n","\n","  future_forecast = ts_model.predict(n_periods=13)\n","\n","  features.loc[(features.Store == store) & (features.CPI.isnull()),'CPI'] = future_forecast\n","  ```\n","\n"]},{"cell_type":"code","metadata":{"id":"3-ImY0cDQQfX","colab_type":"code","colab":{}},"source":["# Using rolling average to fill the missing values in CPI and Unemployment columns\n","stores_list = list(range(1,46))\n","for store in stores_list:\n","  temp_df = features[features['Store'] == store]\n","  temp_df.Date = pd.to_datetime(temp_df.Date)\n","  temp_df = temp_df.set_index('Date')\n","  \n","  features.loc[(features.Store == store),['CPI']] = list(temp_df.CPI.fillna(temp_df.CPI.rolling(14,min_periods=1).mean()))\n","  features.loc[(features.Store == store),['Unemployment']] = list(temp_df.Unemployment.fillna(temp_df.Unemployment.rolling(14,min_periods=1).mean()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WReq2T2zNskw","colab_type":"code","colab":{}},"source":["sales_train['Weekly_Sales'] = sales_train['Weekly_Sales'].abs() "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_0L5SayPtry","colab_type":"text"},"source":["# 2) Feature Engineering\n","\n","\n","*   Since we are going to fit tree-based models, feature scaling is not required. Random Forest/Boosted trees use information gain/gini coefficient inherently which will not be affected by scaling unlike many other machine learning models (such as Linear Regression, PCA)\n","\n","*   We One hot encode \"Type\" categorical feature since it contains more than 2 levels (A/B/C)\n","\n","*   We label encode the \"IsHoliday\" categorical feature since it contains just 2 levels (True/False)\n","\n","*   We create new features namely - \"IsSuperbowl\", \"IsLaborday\", \"IsThanksgiving\", \"IsChristmas\" to identify the type of holiday \n","\n","*   Extract new features from \"Date\" feature such as year, month, week of year, week of month, day etc\n","\n","*   Each of the feature \"Markdown1\", \"Markdown2\", \"Markdown3\", \"Markdown4\", and \"Markdown5\" has around 65% missing date, we ignore these variables from our analysis.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BFWQk7_RLimL","colab_type":"text"},"source":["## 2.1) Merge and join dataframes"]},{"cell_type":"code","metadata":{"id":"gg-RVtqZWN5v","colab_type":"code","colab":{}},"source":["temp1=pd.merge(sales_train, features, how='left')\n","train=pd.merge(temp1, stores, how='left')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G27Ba10dWN5y","colab_type":"code","colab":{}},"source":["temp2=pd.merge(sales_test, features, how='left')\n","test=pd.merge(temp2, stores, how='left')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFWhsr2zoCFE","colab_type":"code","outputId":"cc81bddc-510b-4c18-cfe3-a01dfbfbcb7c","executionInfo":{"status":"ok","timestamp":1583278706281,"user_tz":360,"elapsed":549,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"04844477878113654498"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Concatenating train & test\n","train['train_or_test'] = 'train'\n","test['train_or_test'] = 'test'\n","merged_df = pd.concat([train,test], sort=False)\n","print('Combined merged_df shape:{}'.format(merged_df.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Combined merged_df shape:(536634, 17)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9x7VV5uEpM3u","colab_type":"code","colab":{}},"source":["# Sorting the dataframe by store then Dept then date\n","merged_df = merged_df.sort_values(by=['Store','Dept','Date'], axis=0).reset_index().drop(columns='index')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWes5h0GKIhD","colab_type":"text"},"source":["## 2.2) Categorical Encodings"]},{"cell_type":"code","metadata":{"id":"rZfwqVvOkwar","colab_type":"code","colab":{}},"source":["# One-hot-encode \"Type\" categorical variables \n","merged_df = pd.get_dummies(merged_df, columns=['Type'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhmIVFpNkv3O","colab_type":"code","colab":{}},"source":["# Label-encode \"IsHoliday\" categorical variables \n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","merged_df['IsHoliday'] = le.fit_transform(merged_df['IsHoliday'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtGBhNHUdak3","colab_type":"code","colab":{}},"source":["# Create new features namely - \"IsSuperbowl\", \"IsLaborday\", \"IsThanksgiving\", \"IsChristmas\" \n","def IsSuperbowl(x):\n","  if (x == '2010-02-12') | (x == '2011-02-11') | (x == '2012-02-10') | (x == '2013-02-08'):\n","    return 1\n","  else:\n","    return 0\n","\n","def IsLaborday(x):\n","  if (x == '2010-09-10') | (x == '2011-09-09') | (x == '2012-09-07') | (x == '2013-09-06'):\n","    return 1\n","  else:\n","    return 0\n","\n","def IsThanksgiving(x):\n","  if (x == '2010-11-26') | (x == '2011-11-25') | (x == '2012-11-23') | (x == '2013-11-29'):\n","    return 1\n","  else:\n","    return 0\n","\n","def IsChristmas(x):\n","  if (x == '2010-12-31') | (x == '2011-12-30') | (x == '2012-12-28') | (x == '2013-12-27'):\n","    return 1\n","  else:\n","    return 0\n","\n","merged_df['IsSuperbowl'] = merged_df['Date'].apply(lambda x: IsSuperbowl(x))\n","merged_df['IsLaborday'] = merged_df['Date'].apply(lambda x: IsLaborday(x))\n","merged_df['IsThanksgiving'] = merged_df['Date'].apply(lambda x: IsThanksgiving(x))\n","merged_df['IsChristmas'] = merged_df['Date'].apply(lambda x: IsChristmas(x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NY42QmLyKqgR","colab_type":"text"},"source":["## 2.3) Datetime Features"]},{"cell_type":"code","metadata":{"id":"bS7Gkxb-f2wi","colab_type":"code","colab":{}},"source":["# Extract features from \"Date\" feature\n","merged_df['WeekofMonth'] = merged_df['Date'].apply(lambda x: pendulum.parse(x).week_of_month)\n","merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n","merged_df['Year'] = merged_df['Date'].dt.year\n","merged_df['Month'] = merged_df['Date'].dt.month\n","merged_df['Week'] = merged_df['Date'].dt.week\n","merged_df['Day'] = merged_df['Date'].dt.day"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eke_PZj-rx8x","colab_type":"code","colab":{}},"source":["# Drop features namely \"Markdown1\", \"Markdown2\", \"Markdown3\", \"Markdown4\", and \"Markdown5\"\n","merged_df = merged_df.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtzMs3pFLN-i","colab_type":"text"},"source":["## 2.4) Featuretools\n"]},{"cell_type":"markdown","metadata":{"id":"ZnZAwB7IFA5m","colab_type":"text"},"source":["\n","\n","```\n","'''\n","# https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219\n","# https://towardsdatascience.com/the-hitchhikers-guide-to-feature-extraction-b4c157e96631\n","# https://mlwhiz.com/blog/2019/05/19/feature_extraction/\n","# https://medium.com/@rrfd/simple-automatic-feature-engineering-using-featuretools-in-python-for-classification-b1308040e183\n","# https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/\n","\n","import featuretools as ft\n","\n","# Create new entityset\n","es = ft.EntitySet(id = 'forecasting')\n","\n","# Create an entity from the \"Sales\" dataframe\n","sales_train_ft = sales_train.copy()\n","sales_train_ft[\"index\"] = sales_train_ft[\"Store\"].astype(str) + \"_\" + sales_train_ft[\"Dept\"].astype(str) + \"_\" + sales_train_ft[\"Date\"].astype(str)\n","es = es.entity_from_dataframe(entity_id = 'sales', dataframe = sales_train_ft,\n","                              index = 'index', variable_types = {\"Date\": ft.variable_types.Datetime, \"IsHoliday\": ft.variable_types.Categorical})\n","\n","# Create an entity from the \"Features\" dataframe\n","store_dept_ft = pd.DataFrame(sales_train_ft[['Store',\t'Dept', 'Date']].drop_duplicates().values.tolist(), columns = ['Store', 'Dept', 'Date'])\n","features_ft = features.copy()\n","features_ft = pd.merge(store_dept_ft, features_ft, on=['Store', 'Date'], how='left')\n","features_ft[\"index\"] = features_ft[\"Store\"].astype(str) + \"_\" + features_ft[\"Dept\"].astype(str) + \"_\" + features_ft[\"Date\"].astype(str)\n","es = es.entity_from_dataframe(entity_id=\"features\",\n","                                 dataframe=features_ft,\n","                                 index=\"index\",\n","                               variable_types={\"Date\": ft.variable_types.Datetime})\n","\n","# Create an entity from the \"Stores\" dataframe\n","stores_ft = stores.copy()\n","stores_ft = pd.merge(stores_ft, sales_train_ft[['Store','Dept','Date']], on='Store', how='left')\n","stores_ft[\"index\"] = stores_ft[\"Store\"].astype(str) + \"_\" + stores_ft[\"Dept\"].astype(str) + \"_\" + stores_ft[\"Date\"].astype(str)\n","es = es.entity_from_dataframe(entity_id=\"stores\",\n","            dataframe=stores_ft,\n","            index=\"index\",\n","            variable_types={\"Type\": ft.variable_types.Categorical, \"Date\": ft.variable_types.Datetime})\n","\n","# adding the relationship\n","relationship1 = ft.Relationship(es[\"sales\"][\"index\"],\n","                       es[\"features\"][\"index\"])\n","\n","# Add the relationship to the entity set\n","es = es.add_relationship(relationship1)\n","\n","# adding the relationship\n","relationship2 = ft.Relationship(es[\"sales\"][\"index\"],\n","                       es[\"stores\"][\"index\"])\n","\n","# Add the relationship to the entity set\n","es = es.add_relationship(relationship2)\n","\n","#create features\n","feature_matrix, feature_defs = ft.dfs(entityset=es, target_entity=\"sales\",max_depth = 2)```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HdIo9B9ZK8sB","colab_type":"text"},"source":["## 2.5) Previous sales values  "]},{"cell_type":"code","metadata":{"id":"hg4xOvY4FJJ_","colab_type":"code","colab":{}},"source":["# Features constructed from previous sales values\n","# Create various sales agg features with given agg functions\n","def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):\n","    gpby = df.groupby(gpby_cols)\n","    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)\n","    for agg_name, agg_func in agg_funcs.items():\n","        aggdf = gpby[target_col].agg(agg_func).reset_index()\n","        aggdf.rename(columns={target_col:'Monthly_Sales_'+agg_name}, inplace=True)\n","        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')\n","    return newdf\n","\n","# Creating sales lag features\n","def create_sales_lag_feats(df, gpby_cols, target_col, lags):\n","    gpby = df.groupby(gpby_cols)\n","    for i in lags:\n","        df['_'.join([target_col, 'lag', str(i)])] = gpby[target_col].shift(i).values \n","    return df\n","\n","# Creating sales exponentially weighted mean features\n","def create_sales_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n","    gpby = df.groupby(gpby_cols)\n","    for a in alpha:\n","        for s in shift:\n","            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = gpby[target_col].apply(lambda x: x.shift(52).ewm(alpha=0.95).mean())\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR3T6a5SZqev","colab_type":"code","colab":{}},"source":["# creating sales lag features \n","merged_df_new = create_sales_lag_feats(merged_df, gpby_cols=['Store','Dept'], \n","                                       target_col='Weekly_Sales', lags=[52])\n","\n","# creating ewm features \n","merged_df_new = create_sales_ewm_feats(merged_df_new, gpby_cols=['Store','Dept'], \n","                                       target_col='Weekly_Sales', alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5],\n","                                       shift=[52])\n","\n","# creating sales monthwise aggregated values\n","agg_df = create_sales_agg_monthwise_features(merged_df_new.loc[~(merged_df_new.train_or_test=='test'), :], gpby_cols=['Store','Dept', 'Month'], \n","                                               target_col='Weekly_Sales', \n","                                             agg_funcs={'mean':np.mean, \n","                                              'median':np.median, 'max':np.max, \n","                                              'min':np.min, 'std':np.std})\n","# # Joining agg_df with merged_df_new\n","merged_df_new = merged_df_new.merge(agg_df, on=['Store','Dept', 'Month'], how='left')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zW9ejylTjltE","colab_type":"code","outputId":"ee107fa2-e236-4dd8-e629-3839020480d3","executionInfo":{"status":"ok","timestamp":1583210820335,"user_tz":360,"elapsed":21253,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"04844477878113654498"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Final train and test datasets\n","train = merged_df_new.loc[merged_df_new.train_or_test=='train', :]\n","test = merged_df_new.loc[merged_df_new.train_or_test=='test', :]\n","print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train shape:(421570, 35), Test shape:(115064, 35)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"grYD9RRXCw_8","colab_type":"code","colab":{}},"source":["# merged_df_new.to_csv(os.path.join(DIR, 'Check.csv'))\n","# Lagged columns in the train data contains NaN's and we remove those rows\n","train = train.drop(['Date', 'train_or_test'], axis=1).dropna() \n","\n","# Lagged columns in the test data contains NaN's because of not being able to find previous years sales \n","# Hence we will fill them with 0's\n","test = test.drop(['Date', 'train_or_test', 'Weekly_Sales'], axis=1) .fillna(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyswoFVUpo7_","colab_type":"text"},"source":["# 3) Machine Learning Modelling\n"]},{"cell_type":"markdown","metadata":{"id":"46HqaccCDCWC","colab_type":"text"},"source":["\n","\n","*   If we optimize the model for the training data, then our model will score very well on the training set, but will not be able to generalize to new data, such as in a test set. When a model performs highly on the training set but poorly on the test set, this is known as overfitting, or essentially creating a model that knows the training set very well but cannot be applied to new problems. An overfit model may look impressive on the training set, but will be useless in a real application. Therefore, we tune hyperparameters cross validation.\n","\n","\n","\n","*   Using Scikit-Learn’s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values. The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rcgByEhEmGMe","colab_type":"text"},"source":["## 3.1) By Store & Department combination\n","\n","*   The following store-department combos are present in test set but not in trian set: \n","\n","5-99, 9-99, 10-99, 18-43, 24-43, 25-99, 34-39, 36-30, 37-29, 42-30, 45-39\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"b1_SjDFQi4zr","colab_type":"code","colab":{}},"source":["# Store parameters for regressors\n","# Random Forest parameters\n","rf_params =  {'n_estimators': [100],     # number of trees in the forest\n","              'max_features': ['auto'], # number of features to consider when looking for the best split\n","              'max_depth': [10, 15],        # maximum number of levels in tree\n","              'min_samples_split': [2, 4], # minimum number of samples required to split a node\n","              'min_samples_leaf': [2]} # minimum number of samples required at each leaf node\n","              #'bootstrap': [True, False]} # method for sampling data points (with or without replacement)\n","\n","# Light GBM parameters\n","lgbm_params = {'boosting_type': ['gbdt'],\n","                'num_leaves': [10, 15], # number of leaves in full tree\n","                'max_depth': [5, 10, 15],  # maximum depth of tree. This parameter is used to handle model overfitting. \n","                'learning_rate': [0.001, 0.01, 0.1], # determines the impact of each tree on the final outcome. \n","                                        # LGBM works by starting with an initial estimate which is updated using the output of each tree. \n","                                        # learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003\n","                'n_estimators': [100, 150], # Number of boosted trees to fit.\n","                'objective': ['regression']}\n","\n","# Gradient Boosting parameters\n","gbm_params = {'max_depth': [2],\n","              'n_estimators': [3],\n","              'learning_rate': [0.1]}\n","\n","# Extreme Gradient Boosting parameters\n","xgb_params = {'learning_rate': [0.01, 0.1, 0.15],\n","              'max_depth': [4, 5, 6, 8],\n","              'subsample': [0.8, 0.9],\n","              'gamma': [1],\n","              'colsample_bytree': [0.9, 1]}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"itqNVXf-jDOt","colab_type":"code","colab":{}},"source":["def write_csv(pred_values, file_name):\n","  kaggle_check = pd.DataFrame()\n","  kaggle_check['Id'] = sales_test['Store'].astype(str) + '_' +  sales_test['Dept'].astype(str) + '_' +  sales_test['Date'].astype(str)\n","  kaggle_check['Weekly_Sales'] = pred_values\n","  return kaggle_check.to_csv(os.path.join(DIR, file_name), index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pgWRFduYl8f","colab_type":"code","colab":{}},"source":["# Class to fit models by store+dept combo\n","class TrainByStoreDeptCombo(object):\n","    def __init__(self, clf, seed=0, params=None):\n","        self.clf = clf()\n","        self.params = params\n","\n","    def train_predict(self, train, test):\n","\n","        pred_values = []\n","        # this model is trained on store+dept filtered data\n","        store_dept_list = test[['Store',\t'Dept']].drop_duplicates().values.tolist()\n","\n","        for tempStore, tempDept in store_dept_list:\n","\n","          print ('Store No: ', tempStore, ' Dept No:', tempDept)\n","          x_train = train[(train['Store'] == tempStore) & (train['Dept'] == tempDept)].drop(['Weekly_Sales'], axis=1)\n","          y_train = train[(train['Store'] == tempStore) & (train['Dept'] == tempDept)]['Weekly_Sales']\n","          x_test = test[(test['Store'] == tempStore) & (test['Dept'] == tempDept)]\n","        \n","          # sample size restrictions since RandomizedSearchCV fails if there isn't enough data\n","          # we cannot have number of splits (cv=5) greater than the number of samples. we need a minimum of 5 training samples for a 5-fold cross validation\n","          # hence the first condition: if training samples for store+dept combo is >= 5, we perform cross validation\n","          # if training samples for store+dept combo is >= 1 and < 5, we do not perform cross validation\n","          # but if training samples for store+dept combo is 0, we predict the sales to be average sales for the department  \n","          if len(x_train) >= 5:\n","          \n","            # random search of parameters, using 5 fold cross validation\n","            rgr = RandomizedSearchCV(estimator=self.clf, param_distributions = self.params, scoring='neg_root_mean_squared_error', \n","                                     iid=False, n_jobs=2, n_iter = 10, cv=5, verbose=5)\n","            rgr = rgr.fit(x_train, y_train)\n","\n","            # validate the best model with optimized number of estimators\n","            rgr = rgr.best_estimator_.fit(x_train, y_train)\n","\n","            # predict values\n","            predict_test = rgr.predict(x_test)\n","\n","          elif len(x_train) >= 1: \n","            # fit model\n","            rgr = self.clf.fit(x_train, y_train)\n","\n","            # predict values\n","            predict_test = rgr.predict(x_test)\n","\n","          else:\n","            predict_test = np.repeat(np.average(sales_train[sales_train['Dept'] == tempDept]['Weekly_Sales']), len(x_test))\n","          \n","          # store the predicted values\n","          pred_values.extend(predict_test)\n","\n","        return pred_values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F79xCTtJDQW8","colab_type":"code","colab":{}},"source":["rf = TrainByStoreDeptCombo(clf=RandomForestRegressor, seed=0, params=rf_params)\n","rf_pred = rf.train_predict(train, test)\n","write_csv(rf_pred, 'output_StoreDept_rf.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXD3-HSIDSe8","colab_type":"code","colab":{}},"source":["lgbm = TrainByStoreDeptCombo(clf=lgb.LGBMRegressor, seed=0, params=lgbm_params)\n","lgbm_pred = lgbm.train_predict(train, test)\n","write_csv(lgbm_pred, 'output_StoreDept_lgbm.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQQiVt9TDT48","colab_type":"code","colab":{}},"source":["gbm = TrainByStoreDeptCombo(clf=GradientBoostingRegressor, seed=0, params=gbm_params)\n","gbm_pred = gbm.train_predict(train, test)\n","write_csv(gbm_pred, 'output_StoreDept_gbm.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvUi_E_ly4YT","colab_type":"text"},"source":["## 3.2) By Department"]},{"cell_type":"code","metadata":{"id":"QbwUPioZqZi_","colab_type":"code","colab":{}},"source":["# Class to fit models by department\n","class TrainByDept(object):\n","    def __init__(self, clf, seed=0, params=None):\n","        self.clf = clf()\n","        self.params = params\n","\n","    def train_predict(self, train, test):\n","\n","        pred_values = []\n","        dept_list = test['Dept'].drop_duplicates().values.tolist()\n","\n","        for tempDept in dept_list:\n","\n","          print ('Dept No:', tempDept)\n","          x_train = train[train['Dept'] == tempDept].drop(['Weekly_Sales'], axis=1)\n","          y_train = train[train['Dept'] == tempDept]['Weekly_Sales']\n","          x_test = test[test['Dept'] == tempDept]\n","        \n","          # there are enough number of training samples for each department and hence sample size restictions are not required \n","          # RandomizedSearchCV can applied for each department \n","          # grid search for best parameters\n","          if len(x_train) >= 5:\n","\n","            rgr = RandomizedSearchCV(estimator=self.clf, param_distributions = self.params, scoring='neg_root_mean_squared_error', \n","                                      iid=False, n_jobs=2, n_iter = 10, cv=5, verbose=5)\n","            \n","            rgr = rgr.fit(x_train, y_train)\n","\n","            # validate the best model with optimized number of estimators\n","            rgr = rgr.best_estimator_.fit(x_train, y_train)\n","\n","            # predict values\n","            predict_test = rgr.predict(x_test)           \n","\n","          elif len(x_train) >= 1: \n","            # fit model\n","            rgr = self.clf.fit(x_train, y_train)\n","\n","            # predict values\n","            predict_test = rgr.predict(x_test)\n","\n","          else:\n","            predict_test = np.repeat(np.average(sales_train[sales_train['Dept'] == tempDept]['Weekly_Sales']), len(x_test))\n","\n","          # store the predicted values\n","          pred_values.extend(predict_test)\n","\n","        return pred_values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"po0eHWgeyQRt","colab_type":"code","colab":{}},"source":["rf = TrainByDept(clf=RandomForestRegressor, seed=0, params=rf_params)\n","rf_pred = rf.train_predict(train, test)\n","write_csv(rf_pred, 'output_dept_rf.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAmUeQkc7KGi","colab_type":"text"},"source":["# 4) Time Series Modeling\n","\n","\n","*   In total there are ~3300 store department combinations.\n","\n","*   The training data has gaps in dates. For any store department combinations that has missing dates, I simply used 0 for weekly sales and make sure that any store department combinations has 143 weekly dates.\n"]},{"cell_type":"markdown","metadata":{"id":"K12uonWL8Jgl","colab_type":"text"},"source":["## 4.1) Prophet"]},{"cell_type":"code","metadata":{"id":"OLjrF4D9nklO","colab_type":"code","colab":{}},"source":["from fbprophet import Prophet\n","\n","init_ts = merged_df_new[['Store',\t'Dept',\t'Date',\t'Weekly_Sales', 'train_or_test']]\n","init_ts = init_ts.set_index('Date')\n","ts_store_dept = init_ts[['Store', 'Dept']].drop_duplicates().reset_index(drop=True).values.tolist()\n","\n","ts = []\n","idx = pd.date_range('2/5/2010', '10/26/2012', freq='W-FRI')\n","\n","for store, dept in ts_store_dept:\n","  tmp_ts = init_ts[(init_ts['Store'] == store) & (init_ts['Dept'] == dept) & (init_ts['train_or_test'] == 'train')].reindex(idx)\n","  values = {'Store': store, 'Dept': dept, 'Weekly_Sales': 0, 'train_or_test': 'train'}\n","  tmp_ts = tmp_ts.fillna(value=values)\n","  \n","  ts.append(tmp_ts)\n","  ts.append(init_ts[(init_ts['Store'] == store) & (init_ts['Dept'] == dept) & (init_ts['train_or_test'] == 'test')])\n","\n","ts = pd.concat(ts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJTe3Ids70o0","colab_type":"text"},"source":["Create a holidays dataframe with two columns (holiday and ds) and a row for each occurrence of the holiday. It must include all occurrences of the holiday, both in the past (back as far as the historical data go) and in the future (out as far as the forecast is being made)."]},{"cell_type":"code","metadata":{"id":"Fe0qP00inkwE","colab_type":"code","colab":{}},"source":["Superbowl = pd.DataFrame({\n","  'holiday' : 'superbowl',\n","  'ds' : pd.to_datetime(['2010-02-12','2011-02-11','2012-02-10','2013-02-08'])\n","  })\n","\n","Laborday = pd.DataFrame({\n","  'holiday' : 'laborday',\n","  'ds' : pd.to_datetime(['2010-09-10','2011-09-09','2012-09-07','2013-09-06'])\n","  })\n","\n","Thanksgiving = pd.DataFrame({\n","  'holiday' : 'thanksgiving',\n","  'ds' : pd.to_datetime(['2010-11-26','2011-11-25','2012-11-23','2013-11-29'])\n","  })\n","\n","Christmas = pd.DataFrame({\n","  'holiday' : 'christmas',\n","  'ds' : pd.to_datetime(['2010-12-31','2011-12-30','2012-12-28','2013-12-27'])\n","  })\n","\n","holidays = pd.concat((Superbowl, Laborday, Thanksgiving, Christmas))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-uG4KJ874aa","colab_type":"code","colab":{}},"source":["ts_p = ts.copy()\n","ts_p = ts_p.reset_index().rename(columns = {'index':'Date'})\n","ts_forecast = []\n","store_dept = test[['Store',\t'Dept']].drop_duplicates().values.tolist()\n","\n","for store, dept in store_dept:\n","  \n","  print ('Store No: ', store, ' Dept No:', dept)\n","\n","  # Lets narrow our analysis for a single store+dept combo \n","  df = ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'train')][['Date', 'Weekly_Sales']]\n","\n","  # Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. \n","  df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\\\n","\n","  if len(df) >= 5:\n","    # Create an instance of the Prophet class and then fit our dataframe to it.\n","    prophet = Prophet(holidays=holidays)\n","    prophet.fit(df)\n","\n","    # Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). \n","    # Then specify the number of days to forecast using the periods parameter.\n","    df_forecast = prophet.make_future_dataframe(periods=39, freq='W-FRI')\n","\n","    # Call predict to make a prediction and store it in the forecast dataframe.\n","    df_forecast = prophet.predict(df_forecast)\n","\n","    # Retrieve predictions for the dates in the test set\n","    df_forecast = pd.merge(ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'test')][['Store', 'Dept', 'Date']], \n","                           df_forecast[['ds', 'yhat']], \n","                           left_on='Date', right_on='ds', how = 'left')[['Store', 'Dept', 'Date', 'yhat']]\n","\n","  else:\n","\n","    forecast = np.repeat(np.average(ts_p[(ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'train')]['Weekly_Sales']), \n","                         len(ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'test')]))\n","\n","    df_forecast = ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'test')][['Store', 'Dept', 'Date']]\n","\n","    df_forecast['yhat'] = forecast\n","\n","  ts_forecast.append(df_forecast)\n","\n","ts_forecast = pd.concat(ts_forecast)\n","ts_forecast['Store'] = ts_forecast['Store'].astype(int)\n","ts_forecast['Dept'] = ts_forecast['Dept'].astype(int)\n","ts_forecast['Id'] = ts_forecast['Store'].astype(str) + '_' +  ts_forecast['Dept'].astype(str) + '_' +  ts_forecast['Date'].astype(str)\n","ts_forecast = ts_forecast.rename(columns = {'yhat': 'Weekly_Sales'})\n","ts_forecast = ts_forecast[['Id', 'Weekly_Sales']]\n","ts_forecast.to_csv(os.path.join(DIR, 'Output_Prohpet.csv'), index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K11H3O6x8QU0","colab_type":"text"},"source":["## 4.2) ARIMA"]},{"cell_type":"code","metadata":{"id":"vnhJmI3t74fO","colab_type":"code","colab":{}},"source":["import itertools\n","import statsmodels.api as sm\n","\n","ts_arima = ts.copy()\n","ts_arima = ts_arima.reset_index().rename(columns = {'index':'Date'})\n","ts_forecast = []\n","store_dept_list = test[['Store',\t'Dept']].drop_duplicates().values.tolist()\n","\n","p = d = q = range(0, 2)\n","pdq = list(itertools.product(p, d, q))\n","seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n","\n","for store, dept in store_dept_list:\n","  \n","  print ('Store No: ', store, ' Dept No:', dept)\n","\n","  # Lets narrow our analysis for a single store+dept combo \n","  df = ts_arima[(ts_arima['Store'] == store) &  (ts_arima['Dept'] == dept) & (ts_arima['train_or_test'] == 'train')][['Weekly_Sales']]\n","\n","  if len(df) >= 5:\n","    # The problem with plain ARIMA model is that it does not account for seasonality.\n","    # If the time series exhibits seasonality, then, SARIMA is better as it uses seasonal differencing.\n","    # Create empty list to store search results\n","    order_aic_bic=[]\n","\n","    for param in pdq:\n","      for param_seasonal in seasonal_pdq:\n","        try:\n","          # create and fit ARMA(p,q) model\n","          mod = sm.tsa.statespace.SARIMAX(df,\n","                                          order=param,\n","                                          seasonal_order=param_seasonal,\n","                                          enforce_stationarity=False,\n","                                          enforce_invertibility=False)\n","          results = mod.fit()\n","          print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n","\n","          # Append order and results tuple\n","          order_aic_bic.append(((param,param_seasonal, results.aic, results.bic)))\n","        except:\n","          continue\n","\n","    # Construct DataFrame from order_aic_bic\n","    order_df = pd.DataFrame(order_aic_bic, columns=['pdq', 'seasonal_pdq', 'AIC', 'BIC'])\n","\n","    # Print order_df in order of increasing AIC\n","    #print(order_df.sort_values('AIC'))\n","\n","    # Print order_df in order of increasing BIC\n","    # print(order_df.sort_values('BIC'))  \n","\n","    opti_pdq = order_df.sort_values('AIC')['pdq'].iloc[0]\n","    opti_seasonal_pdq = order_df.sort_values('AIC')['seasonal_pdq'].iloc[0]\n","\n","    # Instantiate the model\n","    model = sm.tsa.statespace.SARIMAX(df,\n","                                      order=opti_pdq,\n","                                      seasonal_order=opti_seasonal_pdq,\n","                                      enforce_stationarity=False,\n","                                      enforce_invertibility=False)\n","\n","    # Fit the model\n","    results = model.fit()\n","\n","    # Print model fit summary\n","    #print(results.summary())\n","\n","    # Generate predictions\n","    forecast = results.get_forecast(steps=39)\n","\n","    # Mean Forecast\n","    future_forecast = forecast.predicted_mean\n","\n","    df_forecast = ts_arima[(ts_arima['Store'] == store) &  (ts_arima['Dept'] == dept) & (ts_arima['train_or_test'] == 'test')][['Store', 'Dept', 'Date']]\n","\n","    df_forecast['yhat'] = future_forecast\n","\n","  else:\n","\n","    forecast = np.repeat(np.average(ts_p[(ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'train')]['Weekly_Sales']), \n","                         len(ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'test')]))\n","\n","    df_forecast = ts_p[(ts_p['Store'] == store) &  (ts_p['Dept'] == dept) & (ts_p['train_or_test'] == 'test')][['Store', 'Dept', 'Date']]\n","\n","    df_forecast['yhat'] = forecast\n","\n","  ts_forecast.append(df_forecast)\n","\n","ts_forecast = pd.concat(ts_forecast)\n","ts_forecast['Store'] = ts_forecast['Store'].astype(int)\n","ts_forecast['Dept'] = ts_forecast['Dept'].astype(int)\n","ts_forecast['Id'] = ts_forecast['Store'].astype(str) + '_' +  ts_forecast['Dept'].astype(str) + '_' +  ts_forecast['Date'].astype(str)\n","ts_forecast = ts_forecast.rename(columns = {'yhat': 'Weekly_Sales'})\n","ts_forecast = ts_forecast[['Id', 'Weekly_Sales']]\n","ts_forecast.to_csv(os.path.join(DIR, 'output_sarima.csv'), index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TEhCbxpto2s","colab_type":"text"},"source":["## 5) Averaged base models \n","\n","*   We just average two models here Random Forest, and Prohpet. \n","*   Of course we could easily add more models in the mix.\n"]},{"cell_type":"code","metadata":{"id":"CkGxEnVqYmJv","colab_type":"code","colab":{}},"source":["def AveragingModels(*args):\n","  predictions = np.column_stack(args)\n","  return np.mean(predictions, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZPjRFSLYmPA","colab_type":"code","colab":{}},"source":["rf_prohpet_pred = AveragingModels(rf_pred,prohpet_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlIIAAu-YmXu","colab_type":"code","colab":{}},"source":["write_csv(rf_prohpet_pred, 'output_rf_prohpet.csv')"],"execution_count":0,"outputs":[]}]}